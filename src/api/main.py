from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification
import torch
import os
import glob

app = FastAPI(title="News Classification API", description="API ph√¢n lo·∫°i tin t·ª©c Ti·∫øng Anh s·ª≠ d·ª•ng DistilBERT")

# --- C·∫§U H√åNH ---
MODEL_NAME = "distilbert-base-uncased"
# B·∫£n ƒë·ªì nh√£n (Ph·∫£i kh·ªõp v·ªõi l√∫c train)
LABEL_MAP = {0: 'World üåç', 1: 'Sports ‚öΩ', 2: 'Business üíº', 3: 'Sci/Tech üöÄ'}


# --- H√ÄM T·ª∞ D√í T√åM MODEL (Tr√°nh l·ªói ƒë∆∞·ªùng d·∫´n) ---
def find_model_path():
    print("üïµÔ∏è‚Äç‚ôÇÔ∏è ƒêang ƒëi t√¨m th∆∞ m·ª•c ch·ª©a model...")
    # L·∫•y th∆∞ m·ª•c g·ªëc d·ª± √°n
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(current_dir))

    # T√¨m file config.json (d·∫•u hi·ªáu nh·∫≠n bi·∫øt model)
    matches = glob.glob(f"{project_root}/**/config.json", recursive=True)

    for match in matches:
        if "distilbert-news-classifier" in match:
            model_dir = os.path.dirname(match)
            print(f"‚úÖ ƒê√É T√åM TH·∫§Y MODEL T·∫†I: {model_dir}")
            return model_dir

    return None


# --- LOAD MODEL (Ch·∫°y 1 l·∫ßn khi kh·ªüi ƒë·ªông API) ---
model_path = find_model_path()
if not model_path:
    raise RuntimeError("‚ùå Kh√¥ng t√¨m th·∫•y model ƒë√£ train! B·∫°n ƒë√£ ch·∫°y train.py ch∆∞a?")

print("üß† ƒêang n·∫°p model v√†o RAM...")
device = "mps" if torch.backends.mps.is_available() else "cpu"  # T·ªëi ∆∞u cho Mac
model = DistilBertForSequenceClassification.from_pretrained(model_path).to(device)
tokenizer = DistilBertTokenizer.from_pretrained(model_path)
print("üöÄ API ƒê√É S·∫¥N S√ÄNG PH·ª§C V·ª§!")


# --- ƒê·ªäNH NGHƒ®A D·ªÆ LI·ªÜU ƒê·∫¶U V√ÄO ---
class NewsRequest(BaseModel):
    text: str


# --- API ENDPOINT ---
@app.post("/predict")
async def predict_news(request: NewsRequest):
    """
    Nh·∫≠n m·ªôt ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Anh -> Tr·∫£ v·ªÅ ch·ªß ƒë·ªÅ d·ª± ƒëo√°n.
    """
    try:
        # 1. Chu·∫©n b·ªã d·ªØ li·ªáu
        inputs = tokenizer(request.text, return_tensors="pt", truncation=True, padding=True, max_length=128).to(device)

        # 2. D·ª± ƒëo√°n (Kh√¥ng t√≠nh ƒë·∫°o h√†m ƒë·ªÉ ti·∫øt ki·ªám RAM)
        with torch.no_grad():
            outputs = model(**inputs)

        # 3. L·∫•y k·∫øt qu·∫£
        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)  # T√≠nh ph·∫ßn trƒÉm t·ª± tin
        prediction_idx = torch.argmax(probs, dim=-1).item()  # L·∫•y v·ªã tr√≠ c√≥ ƒëi·ªÉm cao nh·∫•t
        confidence = probs[0][prediction_idx].item()  # ƒê·ªô t·ª± tin (0.0 -> 1.0)

        label = LABEL_MAP.get(prediction_idx, "Unknown")

        return {
            "topic": label,
            "confidence": f"{confidence:.2%}",  # Chuy·ªÉn th√†nh ph·∫ßn trƒÉm (v√≠ d·ª• 95.5%)
            "raw_text": request.text
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/")
def home():
    return {"message": "Hello! ƒê√¢y l√† API ph√¢n lo·∫°i tin t·ª©c. H√£y g·ªçi endpoint /predict nh√©."}