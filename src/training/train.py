import pandas as pd
import torch
from sklearn.model_selection import train_test_split
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
import os
import glob

# --- Cáº¤U HÃŒNH ---
MODEL_NAME = "distilbert-base-uncased"
NUM_LABELS = 4


def find_file(filename, search_path):
    print(f"ğŸ•µï¸â€â™‚ï¸ Äang Ä‘i tÃ¬m file '{filename}' trong dá»± Ã¡n...")
    matches = glob.glob(f"{search_path}/**/{filename}", recursive=True)
    if matches:
        return matches[0]
    return None


def get_data_path():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(os.path.dirname(current_dir))
    standard_path = os.path.join(project_root, "data", "processed", "news_clean.csv")

    if os.path.exists(standard_path):
        return standard_path

    print(f"âš ï¸ KhÃ´ng tháº¥y file á»Ÿ chá»— chuáº©n, Ä‘ang quÃ©t tÃ¬m...")
    found_path = find_file("news_clean.csv", project_root)
    if found_path:
        print(f"âœ… ÄÃƒ TÃŒM THáº¤Y! File Ä‘ang náº±m á»Ÿ: {found_path}")
        return found_path
    return None


def train():
    print("ğŸ”¥ Báº®T Äáº¦U QUÃ TRÃŒNH HUáº¤N LUYá»†N (FULL POWER)...")

    data_path = get_data_path()
    if not data_path:
        print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u!")
        return

    print(f"ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {data_path}")
    df = pd.read_csv(data_path)

    # --- CHáº¾ Äá»˜ FULL DATA (KhÃ´ng cáº¯t nhá») ---
    print(f"ğŸ“Š Dá»¯ liá»‡u Ä‘áº§u vÃ o thá»±c táº¿: {len(df)} dÃ²ng (Full Dataset)")

    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)

    print("ğŸ”¤ Äang táº£i Tokenizer...")
    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)

    def tokenize_function(examples):
        return tokenizer(examples["text_clean"], padding="max_length", truncation=True, max_length=128)

    train_dataset = Dataset.from_pandas(train_df)
    test_dataset = Dataset.from_pandas(test_df)

    print("âš™ï¸ Äang mÃ£ hÃ³a dá»¯ liá»‡u...")
    tokenized_train = train_dataset.map(tokenize_function, batched=True)
    tokenized_test = test_dataset.map(tokenize_function, batched=True)

    print("ğŸ§  Äang táº£i Model DistilBERT...")
    model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)

    project_root = os.path.dirname(os.path.dirname(os.path.dirname(data_path)))
    if "data" not in project_root:
        project_root = os.getcwd()
    model_dir = os.path.join(project_root, "models", "distilbert-news-classifier")

    training_args = TrainingArguments(
        output_dir="./results",
        eval_strategy="epoch",
        save_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=16,
        num_train_epochs=3,
        weight_decay=0.01,
        load_best_model_at_end=True,
        save_total_limit=2,
        # ÄÃ£ xÃ³a tham sá»‘ use_mps_device gÃ¢y lá»—i
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_train,
        eval_dataset=tokenized_test,
    )

    print("ğŸš€ Äang huáº¥n luyá»‡n... (Sáº½ máº¥t nhiá»u thá»i gian hÆ¡n, hÃ£y kiÃªn nháº«n!)")
    trainer.train()

    print("ğŸ“ Äang cháº¥m Ä‘iá»ƒm...")
    results = trainer.evaluate()
    print(f"ğŸ† Káº¿t quáº£ Loss: {results['eval_loss']}")

    print(f"ğŸ’¾ Äang lÆ°u model vÃ o: {model_dir}")
    model.save_pretrained(model_dir)
    tokenizer.save_pretrained(model_dir)
    print("âœ… HOÃ€N Táº¤T! Model xá»‹n Ä‘Ã£ sáºµn sÃ ng.")


if __name__ == "__main__":
    train()